{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_KqglFq2Y0Et"
   },
   "source": [
    "# Лабораторная работа 5\n",
    "# Transfer learning\n",
    "\n",
    "Выполнил Лисин Роман, М8О-406Б-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "l_HrMZxiY0Ev"
   },
   "source": [
    "### Подробно изучить туториал и написать комментарий к каждой строчке кода (https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9mOggoRY0Ew"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Set cuDNN benchmark mode for faster training\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set matplotlib to interactive mode for faster plotting\n",
    "plt.ion()\n",
    "\n",
    "# Data augmentation and normalization for training, and just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # Randomly crop the image to 224x224\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        # Randomly flip the image horizontally\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # Convert the image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the image using the mean and standard deviation of the ImageNet dataset\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        # Resize the image to 256x256\n",
    "        transforms.Resize(256),\n",
    "        # Crop the center 224x224 region of the image\n",
    "        transforms.CenterCrop(224),\n",
    "        # Convert the image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the image using the mean and standard deviation of the ImageNet dataset\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "\n",
    "# Create the ImageFolder datasets for training and validation\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Create the DataLoader for training and validation\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "# Get the sizes of the datasets\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Get the class names\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Set the device to use for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a function to show an image\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    # Convert the image from a tensor to a numpy array\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # Un-normalize the image using the mean and standard deviation of the ImageNet dataset\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    # Clip the image to be between 0 and 1\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    # Plot the image\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from the batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# Show the grid\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    # Record the time when training starts\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        # Create a path to save the best model parameters\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        # Save the initial model parameters\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        # Initialize the best accuracy to 0\n",
    "        best_acc = 0.0\n",
    "\n",
    "        # Iterate over the specified number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            # Print the current epoch and number of epochs\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                # Initialize the running loss and number of correct predictions\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over the data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    # Move the inputs and labels to the GPU\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    # Track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        # Compute the outputs\n",
    "                        outputs = model(inputs)\n",
    "                        # Get the predicted labels\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        # Compute the loss\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward pass and optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            # Compute the gradients of the loss with respect to the model parameters\n",
    "                            loss.backward()\n",
    "                            # Update the model parameters\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Update the running loss and number of correct predictions\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                # Compute the epoch loss and accuracy\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                # Print the epoch loss and accuracy\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # Check if the current model is better than the best model so far\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    # Update the best accuracy\n",
    "                    best_acc = epoch_acc\n",
    "                    # Save the best model parameters\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            # Print a newline\n",
    "            print()\n",
    "\n",
    "        # Compute the total training time\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # Load the best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function to visualize the model's predictions\n",
    "def visualize_model(model, num_images=6):\n",
    "    # Set the model to evaluation mode\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the number of images shown\n",
    "    images_so_far = 0\n",
    "\n",
    "    # Create a figure\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Iterate over the validation data\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass the inputs through the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class labels\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Iterate over the batch size\n",
    "            for j in range(inputs.size()[0]):\n",
    "                # Increment the number of images shown\n",
    "                images_so_far += 1\n",
    "\n",
    "                # Create a subplot\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "\n",
    "                # Turn off the axes\n",
    "                ax.axis('off')\n",
    "\n",
    "                # Set the title of the subplot\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "\n",
    "                # Show the image\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                # If the number of images shown is equal to the specified number of images, break out of the loop\n",
    "                if images_so_far == num_images:\n",
    "                    # Set the model back to training mode\n",
    "                    model.train(mode=was_training)\n",
    "\n",
    "                    # Return\n",
    "                    return\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "# Load a pre-trained ResNet-18 model\n",
    "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Get the number of features in the last fully connected layer\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Replace the last fully connected layer with a new one with the correct number of outputs\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Move the model to the device\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "# Visualize the model's predictions\n",
    "visualize_model(model_ft)\n",
    "\n",
    "\n",
    "# Load a pre-trained ResNet-18 model\n",
    "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Freeze the parameters of the pre-trained model\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of features in the last fully connected layer\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "\n",
    "# Replace the last fully connected layer with a new one with the correct number of outputs\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Move the model to the device\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)\n",
    "\n",
    "# Visualize the model's predictions\n",
    "visualize_model(model_conv)\n",
    "\n",
    "# Turn off interactive mode for matplotlib\n",
    "plt.ioff()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define a function to visualize the model's predictions on a specific image\n",
    "def visualize_model_predictions(model,img_path):\n",
    "    # Set the model to evaluation mode\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Preprocess the image\n",
    "    img = data_transforms['val'](img)\n",
    "\n",
    "    # Add a batch dimension to the image\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Move the image to the device\n",
    "    img = img.to(device)\n",
    "\n",
    "    # Forward pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "\n",
    "    # Get the predicted class label\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # Create a figure\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Create a subplot\n",
    "    ax = plt.subplot(2,2,1)\n",
    "\n",
    "    # Turn off the axes\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Set the title of the subplot\n",
    "    ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
    "\n",
    "    # Show the image\n",
    "    imshow(img.cpu().data[0])\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "\n",
    "# Visualize the model's predictions on a specific image\n",
    "visualize_model_predictions(\n",
    "    model_conv,\n",
    "    img_path='data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg'\n",
    ")\n",
    "\n",
    "# Turn off interactive mode for matplotlib\n",
    "plt.ioff()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "db2Et_Y0Y0Ez"
   },
   "source": [
    "### Выбрать наборы данных и обосновать его выбор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "y_tvqTiVY0Ez"
   },
   "source": [
    "В качестве набора данных для обучения и тестирования будем использовать датасет Fashion MNIST. Это база данных изображений товаров Zalando, состоящая из обучающего набора из 60 000 примеров и тестового набора из 10 000 примеров. Каждый пример - это изображение размером 28х28 градаций серого, связанное с меткой из 10 классов.\n",
    "\n",
    "С помощью данного датасета можно подготовить модель для решения реальной задачи классификации вещей для интернет-магазинов одежды. Например, если покупатель захочет подобрать вещь по картинке, то с помощью предсказанного класса вещи пользователю можно предложить доступные товары из этой категории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xjb_OOocY0Ez"
   },
   "source": [
    "### Выбрать метрики качества и обосновать их выбор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kzxjJs9bY0Ez"
   },
   "source": [
    "Для оценки качества решения задачи достаточно использовать стандартные метрики - accuracy, loss. Accuracy показывает долю правильно классифицированных вещей, что нужно для данной практической задачи. Loss помогает избежать переобучения и недообучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "U7jmLHGuY0E0"
   },
   "source": [
    "### Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av000_okY0E0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "i1TGbdG3Y0E0"
   },
   "source": [
    "Загружаем датасет Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULEEtMfDY0E0"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YMJqljviY0E0"
   },
   "source": [
    "Подсчёт метрик accuracy и loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_v0iBDXYY0E1"
   },
   "outputs": [],
   "source": [
    "def accuracy_and_loss(model, data_loader, device=\"cuda\"):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss_total += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total, loss_total / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oGNvt1VAY0E1"
   },
   "source": [
    "Используем модель ResNet из pytorch для классификации вещей. Добавляем свёрточный слой, так как изображения вещей чёрно-белые, а ResNet изначально заточена под цветные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0BrvRmiY0E1",
    "outputId": "1facc5ca-5233-4161-c42a-118bc1d8b45d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = models.resnet18()\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "b5a8yWY6Y0E1"
   },
   "source": [
    "Обучение ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Rx3M0nmbdLgk"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pA0aQyBEY0E1",
    "outputId": "e76107f8-0b36-4a60-9aa8-e5371c51ed58"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.2827: : 0it [01:06, ?it/s]\n",
      "Loss: 0.4365: : 0it [00:22, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 1/5, training loss: 0.43204232379158675\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: 0.4365: : 0it [00:23, ?it/s]\n",
      "\n",
      "Loss: 0.3285: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.3179: : 0it [00:02, ?it/s]\u001B[A\n",
      "Loss: 0.3162: : 0it [00:04, ?it/s]\u001B[A\n",
      "Loss: 0.3095: : 0it [00:07, ?it/s]\u001B[A\n",
      "Loss: 0.3110: : 0it [00:09, ?it/s]\u001B[A\n",
      "Loss: 0.3082: : 0it [00:12, ?it/s]\u001B[A\n",
      "Loss: 0.3072: : 0it [00:14, ?it/s]\u001B[A\n",
      "Loss: 0.3069: : 0it [00:17, ?it/s]\u001B[A\n",
      "Loss: 0.3052: : 0it [00:19, ?it/s]\u001B[A\n",
      "Loss: 0.3042: : 0it [00:22, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 2/5, training loss: 0.3032578127796271\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.3042: : 0it [00:23, ?it/s]\n",
      "Loss: 0.2681: : 0it [00:22, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 3/5, training loss: 0.2684093166166531\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.2681: : 0it [00:23, ?it/s]\n",
      "\n",
      "Loss: 0.2294: : 0it [00:02, ?it/s]\u001B[A\n",
      "Loss: 0.2301: : 0it [00:04, ?it/s]\u001B[A\n",
      "Loss: 0.2328: : 0it [00:07, ?it/s]\u001B[A\n",
      "Loss: 0.2365: : 0it [00:09, ?it/s]\u001B[A\n",
      "Loss: 0.2404: : 0it [00:12, ?it/s]\u001B[A\n",
      "Loss: 0.2437: : 0it [00:14, ?it/s]\u001B[A\n",
      "Loss: 0.2441: : 0it [00:16, ?it/s]\u001B[A\n",
      "Loss: 0.2434: : 0it [00:19, ?it/s]\u001B[A\n",
      "Loss: 0.2436: : 0it [00:21, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 4/5, training loss: 0.2437274051802372\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.2436: : 0it [00:23, ?it/s]\n",
      "Loss: 0.2203: : 0it [00:22, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 5/5, training loss: 0.21942233395522465\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \")\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_loss = loss.item()\n",
    "        total_loss += cur_loss\n",
    "        if i % 100 == 0:\n",
    "          progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "    print(f\"\\nEpoch {epoch+1}/5, training loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "v0B31Xo_Y0E1"
   },
   "source": [
    "Подсчёт метрик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "rv9jinTsY0E1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "52fb95bc-6402-4b89-807e-3e0378f8de06"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy: 89.94%, Test Loss: 0.2749\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = accuracy_and_loss(model, test_loader)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.2f}%, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "USOCH_pBY0E2"
   },
   "source": [
    "### Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "S7Mgr3l5Y0E2"
   },
   "source": [
    "Добавим аугментацию данных. Поменяем размер изображений и нормализуем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SfN2DFPpY0E2"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = datasets.FashionMNIST(download=False, root=\"./data\").data.float()\n",
    "transformations = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "C0ZDVewmY0E2"
   },
   "source": [
    "Далее снова обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "dil1cGX5Y0E2"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = models.resnet18()\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
    "model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaYCWg1Onk0f",
    "outputId": "01d51700-7bad-4a62-d5bb-03c0d82734eb"
   },
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \")\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_loss = loss.item()\n",
    "        total_loss += cur_loss\n",
    "        if i % 100 == 0:\n",
    "          progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "    print(f\"\\nEpoch {epoch+1}/5, training loss: {total_loss/len(train_loader)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN7ueOqTnyc4",
    "outputId": "065c00e5-93ce-4c59-d219-6338b234b04c"
   },
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: 0.2203: : 0it [15:07, ?it/s]\n",
      "\n",
      "Loss: 2.3946: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.7493: : 0it [00:22, ?it/s]\u001B[A\n",
      "Loss: 0.6158: : 0it [00:43, ?it/s]\u001B[A\n",
      "Loss: 0.5601: : 0it [01:04, ?it/s]\u001B[A\n",
      "Loss: 0.5218: : 0it [01:26, ?it/s]\u001B[A\n",
      "Loss: 0.4940: : 0it [01:48, ?it/s]\u001B[A\n",
      "Loss: 0.4683: : 0it [02:09, ?it/s]\u001B[A\n",
      "Loss: 0.4470: : 0it [02:31, ?it/s]\u001B[A\n",
      "Loss: 0.4298: : 0it [02:53, ?it/s]\u001B[A\n",
      "Loss: 0.4160: : 0it [03:14, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 1/5, training loss: 0.41054935562712297\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.4160: : 0it [03:22, ?it/s]\n",
      "Loss: 0.2589: : 0it [03:12, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 2/5, training loss: 0.25847844969330314\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: 0.2589: : 0it [03:20, ?it/s]\n",
      "\n",
      "Loss: 0.1894: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.2135: : 0it [00:21, ?it/s]\u001B[A\n",
      "Loss: 0.2131: : 0it [00:43, ?it/s]\u001B[A\n",
      "Loss: 0.2171: : 0it [01:04, ?it/s]\u001B[A\n",
      "Loss: 0.2156: : 0it [01:26, ?it/s]\u001B[A\n",
      "Loss: 0.2159: : 0it [01:50, ?it/s]\u001B[A\n",
      "Loss: 0.2151: : 0it [02:13, ?it/s]\u001B[A\n",
      "Loss: 0.2146: : 0it [02:36, ?it/s]\u001B[A\n",
      "Loss: 0.2133: : 0it [02:58, ?it/s]\u001B[A\n",
      "Loss: 0.2137: : 0it [03:21, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 3/5, training loss: 0.21289755486206077\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.2137: : 0it [03:29, ?it/s]\n",
      "Loss: 0.1879: : 0it [03:17, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 4/5, training loss: 0.18820305226215803\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: 0.1879: : 0it [03:25, ?it/s]\n",
      "\n",
      "Loss: 0.1299: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.1555: : 0it [00:24, ?it/s]\u001B[A\n",
      "Loss: 0.1554: : 0it [00:46, ?it/s]\u001B[A\n",
      "Loss: 0.1612: : 0it [01:08, ?it/s]\u001B[A\n",
      "Loss: 0.1622: : 0it [01:30, ?it/s]\u001B[A\n",
      "Loss: 0.1646: : 0it [01:52, ?it/s]\u001B[A\n",
      "Loss: 0.1657: : 0it [02:13, ?it/s]\u001B[A\n",
      "Loss: 0.1643: : 0it [02:35, ?it/s]\u001B[A\n",
      "Loss: 0.1649: : 0it [02:56, ?it/s]\u001B[A\n",
      "Loss: 0.1650: : 0it [03:21, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 5/5, training loss: 0.16643963331408274\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_acc, test_loss = accuracy_and_loss(model, test_loader)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.2f}%, Test Loss: {test_loss:.4f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pITSaY7JnzXQ",
    "outputId": "c2647ba6-d78d-4fcd-83f7-fd88c30313b5"
   },
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy: 91.93%, Test Loss: 0.2238\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Гипотеза с аугментацией данных оказалась верна, так как удалось увеличить accuracy c 89.94% до 91.93% с её помощью."
   ],
   "metadata": {
    "id": "YdzgYoRur5l5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Архитектура сверточной нейронной сети ResNet"
   ],
   "metadata": {
    "id": "7X22ZRM62CNO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet (Residual Network) - это тип сверточной нейронной сети (CNN), разработанный для решения проблемы деградации градиента, которая часто возникает при обучении очень глубоких нейронных сетей. ResNet использует архитектуру с остаточными блоками, которые позволяют сети учиться на своих ошибках и улучшать свою производительность по мере обучения."
   ],
   "metadata": {
    "id": "VLNkgnw02EJd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Архитектура ResNet** состоит из следующих основных компонентов:\n",
    "\n",
    "**Сверточные слои**: ResNet использует сверточные слои для извлечения признаков из входных данных. Эти слои применяют фильтры к входным данным, чтобы обнаружить шаблоны и особенности.\n",
    "\n",
    "**Слои активации**: После каждого сверточного слоя добавляется слой активации, такой как ReLU, чтобы ввести нелинейность в сеть. Это позволяет сети моделировать сложные функции.\n",
    "\n",
    "**Батч-нормализация**: ResNet также использует батч-нормализацию для стабилизации обучения и ускорения сходимости. Батч-нормализация нормализует входные данные по батчам, что делает их менее чувствительными к колебаниям распределения данных.\n",
    "\n",
    "**Остаточные блоки**: Основным строительным блоком ResNet является остаточный блок. Остаточный блок состоит из двух сверточных слоев, за которыми следуют слои активации и батч-нормализации. Выход остаточного блока суммируется с входными данными, что позволяет сети учиться на разнице между входными и выходными данными.\n",
    "\n",
    "**Глобальный средний пулинг**: После последнего остаточного блока используется глобальный средний пулинг для сведения размерности выходных данных. Это приводит к вектору признаков фиксированной длины, который затем подается на полностью связанный слой для классификации."
   ],
   "metadata": {
    "id": "RYn_0nk32JVl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Остаточные блоки играют решающую роль в архитектуре ResNet. Они позволяют сети обучаться быстрее и достигать более высокой точности, решая проблему деградации градиента. Деградация градиента возникает в глубоких сетях, когда градиенты становятся очень малыми по мере прохождения через сеть, что затрудняет обучение более глубоких слоев.\n",
    "\n",
    "Остаточные блоки облегчают обратное распространение градиентов, поскольку градиенты могут проходить через них напрямую, минуя нелинейности. Это обеспечивает более эффективный поток градиентов и позволяет сети учиться на своих ошибках."
   ],
   "metadata": {
    "id": "7ARQExfo2mix"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Преимущества ResNet\n",
    "\n",
    "**Более высокая точность**: ResNet достигает более высокой точности по сравнению с другими архитектурами CNN на различных задачах классификации изображений.\n",
    "\n",
    "**Быстрая сходимость**: Остаточные блоки позволяют ResNet сходиться быстрее, что сокращает время обучения.\n",
    "\n",
    "**Меньшая склонность к переобучению**: ResNet менее подвержен переобучению, чем другие архитектуры CNN.\n",
    "\n",
    "**Возможность обучения на больших данных**: ResNet может быть эффективно обучен на больших наборах данных с миллионами изображений.\n",
    "\n"
   ],
   "metadata": {
    "id": "d7Vekv-62uoh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Недостатки ResNet\n",
    "\n",
    "**Высокие вычислительные затраты**: ResNet требует больших вычислительных ресурсов для обучения из-за большого количества слоев.\n",
    "\n",
    "**Большой размер модели**: ResNet имеет больший размер модели по сравнению с другими архитектурами CNN.\n",
    "\n",
    "**Сложность настройки**: Настройка ResNet для новых задач может быть более сложной из-за большого количества гиперпараметров."
   ],
   "metadata": {
    "id": "Z7Xq9q2c3KDr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Вариации ResNet\n",
    "\n",
    "Существует несколько вариаций ResNet, разработанных для различных задач и наборов данных. Вот некоторые распространенные варианты:\n",
    "\n",
    "ResNet-18: Состоит из 18 остаточных блоков и используется для небольших наборов данных.\n",
    "\n",
    "ResNet-50: Состоит из 50 остаточных блоков и используется для средних и больших наборов данных.\n",
    "\n",
    "ResNet-101: Состоит из 101 остаточного блока и используется для очень больших наборов данных.\n",
    "\n",
    "ResNet-152: Состоит из 152 остаточных блоков и используется для самых больших наборов данных."
   ],
   "metadata": {
    "id": "Tkoy-1za3kN9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Приложения ResNet\n",
    "\n",
    "ResNet широко используется в различных приложениях, включая:\n",
    "\n",
    "**Классификация изображений**: ResNet является одной из наиболее популярных архитектур для классификации изображений и используется в таких задачах, как распознавание объектов, распознавание лиц и медицинская диагностика.\n",
    "\n",
    "**Обнаружение объектов**: ResNet используется для обнаружения объектов на изображениях и используется в таких приложениях, как автомобили с автоматическим управлением и робототехника.\n",
    "\n",
    "**Сегментация изображений**: ResNet используется для сегментации изображений на различные объекты или области и используется в таких приложениях, как медицинская визуализация и автономное вождение.\n",
    "\n",
    "**Генерация изображений**: ResNet также используется для генерации изображений, например, в приложениях увеличения разрешения и создания изображений на основе текста."
   ],
   "metadata": {
    "id": "ffA5L3zC3_Eu"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bpNB_Yi9Y0E2"
   },
   "source": [
    "### Реализация нейросетевой архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "id": "73kC2GvCr2Ee"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 128),\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 256),\n",
    "            ResBlock(256, 256),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "6wpSIumcrd2h"
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = ResNet()\n",
    "model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOIG88rcsnzu",
    "outputId": "d070fbea-771c-4e86-9b2c-1b41515d8aa5"
   },
   "execution_count": 60,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (res_blocks): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \")\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_loss = loss.item()\n",
    "        total_loss += cur_loss\n",
    "        if i % 100 == 0:\n",
    "          progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "    print(f\"\\nEpoch {epoch+1}/5, training loss: {total_loss/len(train_loader)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R_coM1ks7tv",
    "outputId": "1e99e4c0-94d6-4284-bf2f-ba3b8799f78d"
   },
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.1650: : 0it [09:00, ?it/s]\n",
      "Loss: 0.6521: : 0it [08:38, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 1/5, training loss: 0.643514930439402\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: 0.6521: : 0it [08:59, ?it/s]\n",
      "\n",
      "Loss: 0.3994: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.4264: : 0it [00:58, ?it/s]\u001B[A\n",
      "Loss: 0.4278: : 0it [01:55, ?it/s]\u001B[A\n",
      "Loss: 0.4220: : 0it [02:52, ?it/s]\u001B[A\n",
      "Loss: 0.4133: : 0it [03:50, ?it/s]\u001B[A\n",
      "Loss: 0.4085: : 0it [04:47, ?it/s]\u001B[A\n",
      "Loss: 0.4061: : 0it [05:45, ?it/s]\u001B[A\n",
      "Loss: 0.4048: : 0it [06:42, ?it/s]\u001B[A\n",
      "Loss: 0.3998: : 0it [07:40, ?it/s]\u001B[A\n",
      "Loss: 0.3961: : 0it [08:37, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 2/5, training loss: 0.3940116990603872\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.3961: : 0it [08:58, ?it/s]\n",
      "Loss: 0.3294: : 0it [08:37, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 3/5, training loss: 0.3292567003955211\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Loss: 0.3294: : 0it [08:58, ?it/s]\n",
      "\n",
      "Loss: 0.3203: : 0it [00:00, ?it/s]\u001B[A\n",
      "Loss: 0.3073: : 0it [00:57, ?it/s]\u001B[A\n",
      "Loss: 0.3121: : 0it [01:55, ?it/s]\u001B[A\n",
      "Loss: 0.3077: : 0it [02:52, ?it/s]\u001B[A\n",
      "Loss: 0.3026: : 0it [03:50, ?it/s]\u001B[A\n",
      "Loss: 0.3019: : 0it [04:47, ?it/s]\u001B[A\n",
      "Loss: 0.3030: : 0it [05:44, ?it/s]\u001B[A\n",
      "Loss: 0.3005: : 0it [06:42, ?it/s]\u001B[A\n",
      "Loss: 0.2974: : 0it [07:39, ?it/s]\u001B[A\n",
      "Loss: 0.2957: : 0it [08:37, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 4/5, training loss: 0.295149716487063\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loss: 0.2957: : 0it [08:58, ?it/s]\n",
      "Loss: 0.2733: : 0it [08:43, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 5/5, training loss: 0.2727414288722884\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_acc, test_loss = accuracy_and_loss(model, test_loader)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.2f}%, Test Loss: {test_loss:.4f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLMfYZyg3aqX",
    "outputId": "56844fd4-8526-4ee5-e744-5d82111c6eaf"
   },
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy: 89.36%, Test Loss: 0.2992\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модель ResNet, которую я реализовал самостоятельно, показала accuracy 89.36%. Данный результат хуже, чем у предыдущей модели ResNet из pytorch с accuracy 91.93%. Таким образом, лучшим бейзлайном является ResNet из pytorch с использованием аугментации изображений."
   ],
   "metadata": {
    "id": "eVkab72Y4ofE"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
